{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 75)\n",
      "(90, 75)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\".\\deep-learning-from-scratch\")\n",
    "from common.util import im2col\n",
    "\n",
    "x1 = np.random.rand(1, 3, 7, 7)\n",
    "col1 = im2col(x1, 5, 5, stride=1, pad=0)\n",
    "print(col1.shape)\n",
    "\n",
    "x2 = np.random.rand(10, 3, 7, 7)\n",
    "col2 = im2col(x2, 5, 5, stride=1, pad=0)\n",
    "print(col2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)\n",
    "        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T\n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\n",
    "        \n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        out = np.max(col, axis=1)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28),\n",
    "                 conv_param={\"filter_num\":30, \"filter_size\":5, \"stride\":1, \"pad\":0},\n",
    "                 hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param[\"filter_num\"]\n",
    "        filter_size = conv_param[\"filter_size\"]\n",
    "        filter_stride = conv_param[\"stride\"]\n",
    "        filter_pad = conv_param[\"pad\"]\n",
    "        input_size = input_dim\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) ** 2)\n",
    "        \n",
    "        self.params = {}\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.randn(filter_num, input_dim[0],\n",
    "                                                              filter_size, filter_size)\n",
    "        self.params[\"b1\"] = np.zeros(filter_num)\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params[\"b2\"] = np.zeros(hidden_size)\n",
    "        self.params[\"W3\"] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params[\"b3\"] = np.zeros(output_size)\n",
    "        \n",
    "        self.layers = OrderedDict()\n",
    "        self.layers[\"Conv1\"] = Convolution(self.params[\"W1\"], self.params[\"b1\"],\n",
    "                                           conv_param[\"stride\"], conv_param[\"pad\"])\n",
    "        self.layers[\"Relu1\"] = Relu()\n",
    "        self.layers[\"Pool1\"] = Pooling(pool_h=2, pool_2=2, stride=2)\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\n",
    "        self.layers[\"Relu2\"] = Relu()\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"b3\"])\n",
    "        \n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "    \n",
    "    def gradient(self, x, t):\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        dout = 1\n",
    "        dout = self.lasy_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        grads = {}\n",
    "        grads[\"W1\"] = self.layers[\"Conv1\"].dW\n",
    "        grads[\"b1\"] = self.layers[\"Conv1\"].db\n",
    "        grads[\"W2\"] = self.layers[\"Affine1\"].dW\n",
    "        grads[\"b2\"] = self.layers[\"Affine1\"].db\n",
    "        grads[\"W3\"] = self.layers[\"Affine2\"].dW\n",
    "        grads[\"b3\"] = self.layers[\"Affine2\"].db\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:2.2997569369176047\n",
      "=== epoch:1, train acc:0.147, test acc:0.169 ===\n",
      "train loss:2.2962178160640985\n",
      "train loss:2.29315614014403\n",
      "train loss:2.284084065953304\n",
      "train loss:2.2785411667119515\n",
      "train loss:2.2646334922705025\n",
      "train loss:2.253893936202644\n",
      "train loss:2.243240776555138\n",
      "train loss:2.2194956978828886\n",
      "train loss:2.194803213532788\n",
      "train loss:2.17109528538429\n",
      "train loss:2.1521774136111915\n",
      "train loss:2.131193288803014\n",
      "train loss:2.0425248456934755\n",
      "train loss:2.016206164175953\n",
      "train loss:1.9385928997726567\n",
      "train loss:1.8692067801117793\n",
      "train loss:1.7700881550400343\n",
      "train loss:1.6988319066962407\n",
      "train loss:1.6105613327048849\n",
      "train loss:1.5548369264106272\n",
      "train loss:1.570659097901521\n",
      "train loss:1.5343762662892877\n",
      "train loss:1.416357556323223\n",
      "train loss:1.1947987986821\n",
      "train loss:1.2411774381812237\n",
      "train loss:1.1406493771274944\n",
      "train loss:1.1071079622803994\n",
      "train loss:1.057412830563023\n",
      "train loss:0.9631983406586865\n",
      "train loss:1.0056003765198427\n",
      "train loss:1.0148855091293583\n",
      "train loss:0.812198477998719\n",
      "train loss:0.763846616909767\n",
      "train loss:0.8408248343031123\n",
      "train loss:0.6174214631581632\n",
      "train loss:0.7078879825292627\n",
      "train loss:0.7302163577920254\n",
      "train loss:0.7102664882990716\n",
      "train loss:0.7017335671350754\n",
      "train loss:0.6516125716820053\n",
      "train loss:0.4510168256377308\n",
      "train loss:0.6185905434383785\n",
      "train loss:0.6173354361175636\n",
      "train loss:0.6113856759434918\n",
      "train loss:0.543777937532977\n",
      "train loss:0.4243028097431019\n",
      "train loss:0.5508639697230904\n",
      "train loss:0.6474623896858158\n",
      "train loss:0.47114259755591836\n",
      "train loss:0.4888867938113223\n",
      "=== epoch:2, train acc:0.814, test acc:0.795 ===\n",
      "train loss:0.5403169274637077\n",
      "train loss:0.6872570836407889\n",
      "train loss:0.3978878897934463\n",
      "train loss:0.7332761974434803\n",
      "train loss:0.4929547190454403\n",
      "train loss:0.5525722685015758\n",
      "train loss:0.5175407994007974\n",
      "train loss:0.37832307477777016\n",
      "train loss:0.3899601318492478\n",
      "train loss:0.3759247168021364\n",
      "train loss:0.33204848223517514\n",
      "train loss:0.583225037473773\n",
      "train loss:0.4996332852217437\n",
      "train loss:0.3747293515569383\n",
      "train loss:0.48799492394124333\n",
      "train loss:0.49350966176429467\n",
      "train loss:0.3659274418516882\n",
      "train loss:0.4952214626963724\n",
      "train loss:0.5083118491330011\n",
      "train loss:0.40056629831317137\n",
      "train loss:0.4090897429963459\n",
      "train loss:0.4271827290720596\n",
      "train loss:0.27767570332492725\n",
      "train loss:0.46406217187426224\n",
      "train loss:0.26502552407259167\n",
      "train loss:0.30115309792925987\n",
      "train loss:0.4339645770504137\n",
      "train loss:0.5920126565637953\n",
      "train loss:0.3894449071399146\n",
      "train loss:0.349036950171986\n",
      "train loss:0.24051804207515204\n",
      "train loss:0.3863521181290943\n",
      "train loss:0.45252927936703347\n",
      "train loss:0.5147955553483026\n",
      "train loss:0.37729905227579175\n",
      "train loss:0.2628601599191064\n",
      "train loss:0.2525972120923715\n",
      "train loss:0.2536958242321117\n",
      "train loss:0.3303983694074193\n",
      "train loss:0.4090786845428785\n",
      "train loss:0.31475622473986725\n",
      "train loss:0.3507165241989687\n",
      "train loss:0.3645001746228764\n",
      "train loss:0.3038538366952351\n",
      "train loss:0.2942450178255101\n",
      "train loss:0.20473856471834967\n",
      "train loss:0.37081752529270273\n",
      "train loss:0.47672759051270397\n",
      "train loss:0.3844182944348819\n",
      "train loss:0.2952444785035715\n",
      "=== epoch:3, train acc:0.879, test acc:0.865 ===\n",
      "train loss:0.24917105446853793\n",
      "train loss:0.3207932404891774\n",
      "train loss:0.29943443893977995\n",
      "train loss:0.29193059534552596\n",
      "train loss:0.42556162813538195\n",
      "train loss:0.31441202761248027\n",
      "train loss:0.3174813903583585\n",
      "train loss:0.24905790689818894\n",
      "train loss:0.34681595901588286\n",
      "train loss:0.2507035472493663\n",
      "train loss:0.27392923384120654\n",
      "train loss:0.18348432964486444\n",
      "train loss:0.4616382889048304\n",
      "train loss:0.29268966977830324\n",
      "train loss:0.3172262342397756\n",
      "train loss:0.30875452225960864\n",
      "train loss:0.3136863131427035\n",
      "train loss:0.2759148197529242\n",
      "train loss:0.5756525191888529\n",
      "train loss:0.3152443957045033\n",
      "train loss:0.2936333241798894\n",
      "train loss:0.2797883400598338\n",
      "train loss:0.47740328059013065\n",
      "train loss:0.2962494012370699\n",
      "train loss:0.4089467019591229\n",
      "train loss:0.3482579304872464\n",
      "train loss:0.21190605222522124\n",
      "train loss:0.27763866986915486\n",
      "train loss:0.3410167052678966\n",
      "train loss:0.2507060010438272\n",
      "train loss:0.2765948182928156\n",
      "train loss:0.2113391367032796\n",
      "train loss:0.3745576528150447\n",
      "train loss:0.33695808227332746\n",
      "train loss:0.2888953775525885\n",
      "train loss:0.17635055534267294\n",
      "train loss:0.26146850409852385\n",
      "train loss:0.32035750170928823\n",
      "train loss:0.11486381648575099\n",
      "train loss:0.3795230255981773\n",
      "train loss:0.23118442912526954\n",
      "train loss:0.2421682115520703\n",
      "train loss:0.37742237964215364\n",
      "train loss:0.24110572117433432\n",
      "train loss:0.3558485927784744\n",
      "train loss:0.3158597823260661\n",
      "train loss:0.3299051896841837\n",
      "train loss:0.424711803632817\n",
      "train loss:0.28519068210478543\n",
      "train loss:0.3891219998755038\n",
      "=== epoch:4, train acc:0.893, test acc:0.891 ===\n",
      "train loss:0.2802486671316608\n",
      "train loss:0.16487558712947326\n",
      "train loss:0.30048164952592255\n",
      "train loss:0.3314693234963949\n",
      "train loss:0.3266526056614897\n",
      "train loss:0.2821906170504657\n",
      "train loss:0.2845564119959593\n",
      "train loss:0.224097835925707\n",
      "train loss:0.288718939447824\n",
      "train loss:0.22095290787735294\n",
      "train loss:0.29463079397651387\n",
      "train loss:0.2781866179654103\n",
      "train loss:0.18917126616904079\n",
      "train loss:0.2844632078809372\n",
      "train loss:0.2555322799332651\n",
      "train loss:0.27545246779038374\n",
      "train loss:0.275818577895133\n",
      "train loss:0.3523306755525605\n",
      "train loss:0.1942244017702047\n",
      "train loss:0.25295258571702023\n",
      "train loss:0.32637796323566654\n",
      "train loss:0.3018737841302732\n",
      "train loss:0.32119741054931\n",
      "train loss:0.23843692042281092\n",
      "train loss:0.3103869058647117\n",
      "train loss:0.20653104384823454\n",
      "train loss:0.3518526780268442\n",
      "train loss:0.25126937406891925\n",
      "train loss:0.26612298759373404\n",
      "train loss:0.3066894062116042\n",
      "train loss:0.21959007254934806\n",
      "train loss:0.17067669400484312\n",
      "train loss:0.2818957665165092\n",
      "train loss:0.24739640845134261\n",
      "train loss:0.18618352015773967\n",
      "train loss:0.2910327678227523\n",
      "train loss:0.17333750839425263\n",
      "train loss:0.27902317276403826\n",
      "train loss:0.2782560056793313\n",
      "train loss:0.15439195761206717\n",
      "train loss:0.3026809293523282\n",
      "train loss:0.1759975915520223\n",
      "train loss:0.29364256166191216\n",
      "train loss:0.3837513675033993\n",
      "train loss:0.3095619833601772\n",
      "train loss:0.18233104809991393\n",
      "train loss:0.38868216558839125\n",
      "train loss:0.24629759891247485\n",
      "train loss:0.14686628597890178\n",
      "train loss:0.2172650256306634\n",
      "=== epoch:5, train acc:0.914, test acc:0.897 ===\n",
      "train loss:0.2604885409063737\n",
      "train loss:0.21542419679696856\n",
      "train loss:0.1919373437768538\n",
      "train loss:0.2763679239429963\n",
      "train loss:0.32345993607246926\n",
      "train loss:0.23315872601226495\n",
      "train loss:0.24999158695670332\n",
      "train loss:0.17416224585107934\n",
      "train loss:0.1760368791569112\n",
      "train loss:0.28484812982519914\n",
      "train loss:0.14604314597724252\n",
      "train loss:0.16066841969300213\n",
      "train loss:0.15445242927129968\n",
      "train loss:0.28660815855574917\n",
      "train loss:0.16211023133918664\n",
      "train loss:0.15660541499143743\n",
      "train loss:0.16254523291748182\n",
      "train loss:0.1785047034568014\n",
      "train loss:0.15794683462300835\n",
      "train loss:0.1670783425842048\n",
      "train loss:0.2128110978192578\n",
      "train loss:0.1282963027124101\n",
      "train loss:0.22643916801520064\n",
      "train loss:0.12297450309979455\n",
      "train loss:0.21018257069245777\n",
      "train loss:0.20682302389348972\n",
      "train loss:0.18839581004745615\n",
      "train loss:0.18336484472002798\n",
      "train loss:0.09934929681426631\n",
      "train loss:0.20113277047930567\n",
      "train loss:0.1360067916970833\n",
      "train loss:0.19517238457484576\n",
      "train loss:0.21552372286033986\n",
      "train loss:0.08937577543228492\n",
      "train loss:0.193449802104686\n",
      "train loss:0.24516827489884313\n",
      "train loss:0.1695931228845159\n",
      "train loss:0.2850561974703199\n",
      "train loss:0.27134098910920484\n",
      "train loss:0.16468949492999305\n",
      "train loss:0.13264238393622413\n",
      "train loss:0.26726617720564544\n",
      "train loss:0.14413136054704287\n",
      "train loss:0.24983218868960855\n",
      "train loss:0.17220981459069404\n",
      "train loss:0.15230243336331745\n",
      "train loss:0.17313478221630826\n",
      "train loss:0.14412413539056737\n",
      "train loss:0.1123481234960766\n",
      "train loss:0.1697241097362115\n",
      "=== epoch:6, train acc:0.938, test acc:0.913 ===\n",
      "train loss:0.13876711293324154\n",
      "train loss:0.1425634206808609\n",
      "train loss:0.15861185717746903\n",
      "train loss:0.0966464303635272\n",
      "train loss:0.19268437859537665\n",
      "train loss:0.2136517467964292\n",
      "train loss:0.26926374455157837\n",
      "train loss:0.10372163674472176\n",
      "train loss:0.2030454875538304\n",
      "train loss:0.17076682261108034\n",
      "train loss:0.13137637680927847\n",
      "train loss:0.15183818172650856\n",
      "train loss:0.1700634238703166\n",
      "train loss:0.1267371677345249\n",
      "train loss:0.09798505296455941\n",
      "train loss:0.12070751278347507\n",
      "train loss:0.10864256948766565\n",
      "train loss:0.11535268575340113\n",
      "train loss:0.22224349268031512\n",
      "train loss:0.22664527030759438\n",
      "train loss:0.2074965098366322\n",
      "train loss:0.1108190554742737\n",
      "train loss:0.12160529589827895\n",
      "train loss:0.13766533928204017\n",
      "train loss:0.17253267941656247\n",
      "train loss:0.1154296034664168\n",
      "train loss:0.12740190508296995\n",
      "train loss:0.23516326813261432\n",
      "train loss:0.28812157449019393\n",
      "train loss:0.08187676810219183\n",
      "train loss:0.08364865633287469\n",
      "train loss:0.16376087458234015\n",
      "train loss:0.11937992619366154\n",
      "train loss:0.12471448639360888\n",
      "train loss:0.25115734658094907\n",
      "train loss:0.16514337849855018\n",
      "train loss:0.168660111175312\n",
      "train loss:0.22850679062633333\n",
      "train loss:0.212714415985933\n",
      "train loss:0.10032359784087465\n",
      "train loss:0.20483417604197007\n",
      "train loss:0.134793483028841\n",
      "train loss:0.16118616405987982\n",
      "train loss:0.15002601518675818\n",
      "train loss:0.17573725505033874\n",
      "train loss:0.14060483909879046\n",
      "train loss:0.15007119936368749\n",
      "train loss:0.1576734356958946\n",
      "train loss:0.18092166232428464\n",
      "train loss:0.23103070131716152\n",
      "=== epoch:7, train acc:0.945, test acc:0.931 ===\n",
      "train loss:0.13375329602156819\n",
      "train loss:0.0978689601539131\n",
      "train loss:0.1435513493682501\n",
      "train loss:0.1817272060192567\n",
      "train loss:0.1821533367750928\n",
      "train loss:0.07115178721743647\n",
      "train loss:0.09599719206848341\n",
      "train loss:0.11813293805708482\n",
      "train loss:0.08124190259055272\n",
      "train loss:0.14361400705290323\n",
      "train loss:0.12127087631665995\n",
      "train loss:0.08799827194047953\n",
      "train loss:0.15326567903172394\n",
      "train loss:0.06146047554738607\n",
      "train loss:0.15577692650302477\n",
      "train loss:0.10094128356818867\n",
      "train loss:0.07311412037808332\n",
      "train loss:0.1683321453937359\n",
      "train loss:0.09952674670250275\n",
      "train loss:0.118948956338965\n",
      "train loss:0.08507465011569879\n",
      "train loss:0.18635213793971908\n",
      "train loss:0.11378550018442787\n",
      "train loss:0.1254914923004128\n",
      "train loss:0.13446392348814362\n",
      "train loss:0.06436191784694235\n",
      "train loss:0.12270929243666095\n",
      "train loss:0.14803326429459357\n",
      "train loss:0.07063146935427181\n",
      "train loss:0.20473055056108833\n",
      "train loss:0.1597213520146067\n",
      "train loss:0.13792786263514192\n",
      "train loss:0.13734894086168475\n",
      "train loss:0.1220267671692892\n",
      "train loss:0.27281067146657695\n",
      "train loss:0.11433616168732351\n",
      "train loss:0.16480624403262037\n",
      "train loss:0.12796552814026402\n",
      "train loss:0.23355414646276326\n",
      "train loss:0.15257435332978417\n",
      "train loss:0.11261476744774479\n",
      "train loss:0.2631365105972286\n",
      "train loss:0.16576955480096284\n",
      "train loss:0.09205183690677204\n",
      "train loss:0.12686282399245957\n",
      "train loss:0.14653536494263053\n",
      "train loss:0.13504082156840824\n",
      "train loss:0.11122473567338492\n",
      "train loss:0.19752518666949023\n",
      "train loss:0.06416951062546902\n",
      "=== epoch:8, train acc:0.952, test acc:0.943 ===\n",
      "train loss:0.20357281425733398\n",
      "train loss:0.142258614608962\n",
      "train loss:0.1505704748797379\n",
      "train loss:0.18271129520483484\n",
      "train loss:0.12328835575641744\n",
      "train loss:0.11195473722673258\n",
      "train loss:0.05784854208954611\n",
      "train loss:0.07203440858449647\n",
      "train loss:0.12154568266122306\n",
      "train loss:0.3007104517549605\n",
      "train loss:0.1396322449523598\n",
      "train loss:0.0749219780574471\n",
      "train loss:0.0699406743238894\n",
      "train loss:0.20663303999690266\n",
      "train loss:0.14322226165262653\n",
      "train loss:0.1626798472605353\n",
      "train loss:0.12757835535121642\n",
      "train loss:0.1547361384280129\n",
      "train loss:0.16645152059128823\n",
      "train loss:0.12959604239712952\n",
      "train loss:0.09265803776747587\n",
      "train loss:0.1588854325483028\n",
      "train loss:0.12219151705840799\n",
      "train loss:0.05323285039864035\n",
      "train loss:0.08757436916711286\n",
      "train loss:0.08966079931448034\n",
      "train loss:0.2531218450363544\n",
      "train loss:0.12358575978542287\n",
      "train loss:0.1703569047427668\n",
      "train loss:0.22467831513625783\n",
      "train loss:0.06810164643221014\n",
      "train loss:0.09304379986733878\n",
      "train loss:0.17044605316509834\n",
      "train loss:0.171359165163982\n",
      "train loss:0.09223006401526583\n",
      "train loss:0.06162953688977339\n",
      "train loss:0.09920683180094667\n",
      "train loss:0.17070404177056386\n",
      "train loss:0.2569663191226865\n",
      "train loss:0.09358232384015494\n",
      "train loss:0.08852933180160966\n",
      "train loss:0.08229499157388206\n",
      "train loss:0.07267859108854346\n",
      "train loss:0.1309793969637656\n",
      "train loss:0.07320017627690831\n",
      "train loss:0.07583918782646225\n",
      "train loss:0.11746224385663678\n",
      "train loss:0.13433212861610655\n",
      "train loss:0.10592231666048456\n",
      "train loss:0.15179309983572356\n",
      "=== epoch:9, train acc:0.958, test acc:0.936 ===\n",
      "train loss:0.10588911945904092\n",
      "train loss:0.18484685194063524\n",
      "train loss:0.16526340714039559\n",
      "train loss:0.152779194341423\n",
      "train loss:0.12434831130304574\n",
      "train loss:0.093929248023956\n",
      "train loss:0.13070361324370178\n",
      "train loss:0.15649229505983586\n",
      "train loss:0.1642539497557685\n",
      "train loss:0.18759199724996356\n",
      "train loss:0.06828353849972568\n",
      "train loss:0.09182823722694428\n",
      "train loss:0.15565546542598319\n",
      "train loss:0.08040679822850358\n",
      "train loss:0.0790493739880773\n",
      "train loss:0.10825563740935103\n",
      "train loss:0.07720382326516192\n",
      "train loss:0.09665008433828502\n",
      "train loss:0.11217962460520986\n",
      "train loss:0.1524937299433748\n",
      "train loss:0.1263707153285376\n",
      "train loss:0.10310820385392658\n",
      "train loss:0.07968340855786749\n",
      "train loss:0.15239009586521057\n",
      "train loss:0.0715302028777263\n",
      "train loss:0.12396761261063823\n",
      "train loss:0.06948354557028122\n",
      "train loss:0.111093797842778\n",
      "train loss:0.09991070093968123\n",
      "train loss:0.08016525628470872\n",
      "train loss:0.11293112995579238\n",
      "train loss:0.1156777511290399\n",
      "train loss:0.07525620221205323\n",
      "train loss:0.08679066856602308\n",
      "train loss:0.11198085790661055\n",
      "train loss:0.08234075515424616\n",
      "train loss:0.10708431861180942\n",
      "train loss:0.07833906816811963\n",
      "train loss:0.0748489354401297\n",
      "train loss:0.0812464916914036\n",
      "train loss:0.085985840339861\n",
      "train loss:0.09955625824049445\n",
      "train loss:0.07934180391064624\n",
      "train loss:0.09957301748366913\n",
      "train loss:0.12959509862598315\n",
      "train loss:0.12927447719350996\n",
      "train loss:0.07071483537661134\n",
      "train loss:0.0855182504189429\n",
      "train loss:0.071501427684928\n",
      "train loss:0.12230920692914297\n",
      "=== epoch:10, train acc:0.956, test acc:0.939 ===\n",
      "train loss:0.05618117412049115\n",
      "train loss:0.07122934863624453\n",
      "train loss:0.053901402170987024\n",
      "train loss:0.10845777923215058\n",
      "train loss:0.09470088048244024\n",
      "train loss:0.05957401028883693\n",
      "train loss:0.07797483815393698\n",
      "train loss:0.10016149802533683\n",
      "train loss:0.08140157673738464\n",
      "train loss:0.08810187584342243\n",
      "train loss:0.05476886763574289\n",
      "train loss:0.14911895387558632\n",
      "train loss:0.16665032781166353\n",
      "train loss:0.0569096525452275\n",
      "train loss:0.07898731671362468\n",
      "train loss:0.09346283534729972\n",
      "train loss:0.15527391816628783\n",
      "train loss:0.06553468479210181\n",
      "train loss:0.08811593939919042\n",
      "train loss:0.129140617077952\n",
      "train loss:0.1065124473682656\n",
      "train loss:0.048356048698850004\n",
      "train loss:0.2293832258622953\n",
      "train loss:0.0547151758591834\n",
      "train loss:0.082371693680027\n",
      "train loss:0.030955596216115103\n",
      "train loss:0.10650171191941898\n",
      "train loss:0.09460541485680393\n",
      "train loss:0.03651054848921894\n",
      "train loss:0.08131254116081857\n",
      "train loss:0.03974674430122552\n",
      "train loss:0.03509462603578452\n",
      "train loss:0.0841378668180684\n",
      "train loss:0.04858815823228062\n",
      "train loss:0.09973617711853378\n",
      "train loss:0.04892973969815523\n",
      "train loss:0.0921772285541161\n",
      "train loss:0.07592071548970752\n",
      "train loss:0.07612905925785637\n",
      "train loss:0.07917013567184668\n",
      "train loss:0.033421118150000664\n",
      "train loss:0.07390135635253227\n",
      "train loss:0.03954927752788806\n",
      "train loss:0.0838972367859408\n",
      "train loss:0.05081949385757855\n",
      "train loss:0.03278266544994099\n",
      "train loss:0.06596479725389887\n",
      "train loss:0.12172771950504872\n",
      "train loss:0.08915284618654144\n",
      "train loss:0.07527897680239622\n",
      "=== epoch:11, train acc:0.965, test acc:0.949 ===\n",
      "train loss:0.07597063338889204\n",
      "train loss:0.12597045606307483\n",
      "train loss:0.06812798205785073\n",
      "train loss:0.04901329569998174\n",
      "train loss:0.17058271268884082\n",
      "train loss:0.0719516694518048\n",
      "train loss:0.04083483323336906\n",
      "train loss:0.06355539252259819\n",
      "train loss:0.026272429668561623\n",
      "train loss:0.161804326330294\n",
      "train loss:0.03885561909370375\n",
      "train loss:0.12380889230446351\n",
      "train loss:0.0623432143635192\n",
      "train loss:0.10116443512511833\n",
      "train loss:0.08075360373850177\n",
      "train loss:0.06695827511687918\n",
      "train loss:0.07187529006994801\n",
      "train loss:0.08262647184398178\n",
      "train loss:0.04179620929744125\n",
      "train loss:0.13180653220178193\n",
      "train loss:0.07554846905328666\n",
      "train loss:0.10857536041272554\n",
      "train loss:0.11235353141033294\n",
      "train loss:0.057827646911678555\n",
      "train loss:0.07586743281723904\n",
      "train loss:0.1785359552935716\n",
      "train loss:0.053573402458243914\n",
      "train loss:0.05345624439071775\n",
      "train loss:0.06452388427656014\n",
      "train loss:0.11115628730109582\n",
      "train loss:0.06575666111624179\n",
      "train loss:0.08849943106977558\n",
      "train loss:0.08737019835877179\n",
      "train loss:0.06320173509089666\n",
      "train loss:0.06118776356075666\n",
      "train loss:0.07278301606584513\n",
      "train loss:0.11635873490162321\n",
      "train loss:0.06492306762041682\n",
      "train loss:0.06790059625836412\n",
      "train loss:0.06643764680212726\n",
      "train loss:0.06544145973208958\n",
      "train loss:0.08471248898163301\n",
      "train loss:0.11853772463214352\n",
      "train loss:0.0927463474712602\n",
      "train loss:0.09012149588956037\n",
      "train loss:0.06985505892867137\n",
      "train loss:0.06428920945178841\n",
      "train loss:0.04583265623249881\n",
      "train loss:0.09147951186661958\n",
      "train loss:0.08664577707486164\n",
      "=== epoch:12, train acc:0.971, test acc:0.949 ===\n",
      "train loss:0.05765585586970932\n",
      "train loss:0.06427481090764754\n",
      "train loss:0.06643374968824581\n",
      "train loss:0.07224977183303598\n",
      "train loss:0.05644222274997713\n",
      "train loss:0.05389623826230506\n",
      "train loss:0.08878356080312137\n",
      "train loss:0.04636140442218241\n",
      "train loss:0.09021043179158968\n",
      "train loss:0.08452348748845145\n",
      "train loss:0.08228379496691765\n",
      "train loss:0.1010159915145387\n",
      "train loss:0.07232218782720098\n",
      "train loss:0.12930816141102927\n",
      "train loss:0.07310749687994564\n",
      "train loss:0.08105542225722347\n",
      "train loss:0.10227443579574785\n",
      "train loss:0.0949564295023696\n",
      "train loss:0.1259447680113319\n",
      "train loss:0.10703651056842882\n",
      "train loss:0.05969432671229182\n",
      "train loss:0.03460253983841235\n",
      "train loss:0.08292467084137801\n",
      "train loss:0.0849306345783417\n",
      "train loss:0.0517659912104132\n",
      "train loss:0.046600881771346934\n",
      "train loss:0.08401036940820708\n",
      "train loss:0.0719775721694881\n",
      "train loss:0.11629231046823517\n",
      "train loss:0.07603342385015831\n",
      "train loss:0.034895453405288955\n",
      "train loss:0.08130642798610181\n",
      "train loss:0.16239275616542695\n",
      "train loss:0.02088211776685285\n",
      "train loss:0.08601476882513572\n",
      "train loss:0.0411942340686772\n",
      "train loss:0.07983569250015758\n",
      "train loss:0.04210719991825358\n",
      "train loss:0.05894767175758028\n",
      "train loss:0.0438099400342987\n",
      "train loss:0.07552355928642171\n",
      "train loss:0.03953998264387308\n",
      "train loss:0.05880702229288445\n",
      "train loss:0.031898803944314776\n",
      "train loss:0.039815982178674394\n",
      "train loss:0.028775240540622277\n",
      "train loss:0.07913384760060689\n",
      "train loss:0.0726489112838058\n",
      "train loss:0.05680077676974903\n",
      "train loss:0.03401234591829463\n",
      "=== epoch:13, train acc:0.977, test acc:0.951 ===\n",
      "train loss:0.036106160266990046\n",
      "train loss:0.02696611402284821\n",
      "train loss:0.028035132174194777\n",
      "train loss:0.06699731631700988\n",
      "train loss:0.06500344962337831\n",
      "train loss:0.05680368662955715\n",
      "train loss:0.025432637022338417\n",
      "train loss:0.090121364581834\n",
      "train loss:0.03120305777800181\n",
      "train loss:0.12420555386993597\n",
      "train loss:0.06042116193151282\n",
      "train loss:0.055036760980824134\n",
      "train loss:0.12492163326589262\n",
      "train loss:0.02719767023143542\n",
      "train loss:0.033545414551161105\n",
      "train loss:0.07221386833162975\n",
      "train loss:0.10403209432047487\n",
      "train loss:0.07207617746816013\n",
      "train loss:0.022699798818928047\n",
      "train loss:0.009575193736999242\n",
      "train loss:0.07187134327728431\n",
      "train loss:0.094551127331806\n",
      "train loss:0.023556567225289697\n",
      "train loss:0.12228642514942817\n",
      "train loss:0.08342312437189203\n",
      "train loss:0.11727565474304973\n",
      "train loss:0.06843914904604642\n",
      "train loss:0.011616922779800323\n",
      "train loss:0.08420105834289364\n",
      "train loss:0.02620621504892878\n",
      "train loss:0.092019037343172\n",
      "train loss:0.15418949105604088\n",
      "train loss:0.044444631800778074\n",
      "train loss:0.07383507717517056\n",
      "train loss:0.0317770193472417\n",
      "train loss:0.0782095806743686\n",
      "train loss:0.12211973868016156\n",
      "train loss:0.0245600587690973\n",
      "train loss:0.023360421323062396\n",
      "train loss:0.06606285021668072\n",
      "train loss:0.023047998230811783\n",
      "train loss:0.03757466999791982\n",
      "train loss:0.05305285401336451\n",
      "train loss:0.03967408492402964\n",
      "train loss:0.06311422253160695\n",
      "train loss:0.04848850246979101\n",
      "train loss:0.1001605967988856\n",
      "train loss:0.03157116411585371\n",
      "train loss:0.0393824393975273\n",
      "train loss:0.04199918231400363\n",
      "=== epoch:14, train acc:0.978, test acc:0.953 ===\n",
      "train loss:0.025573078471535705\n",
      "train loss:0.020395708580062295\n",
      "train loss:0.10757005952601802\n",
      "train loss:0.028240578430305002\n",
      "train loss:0.05911385081302521\n",
      "train loss:0.09521938800768329\n",
      "train loss:0.034258747192195654\n",
      "train loss:0.0436641059016456\n",
      "train loss:0.012706589628978839\n",
      "train loss:0.12043226960241406\n",
      "train loss:0.03363737316778601\n",
      "train loss:0.025893324468864515\n",
      "train loss:0.05454886621031481\n",
      "train loss:0.05736166281125685\n",
      "train loss:0.0497938672985161\n",
      "train loss:0.14955723272332416\n",
      "train loss:0.041637567882301024\n",
      "train loss:0.068736810634437\n",
      "train loss:0.03364929872770388\n",
      "train loss:0.05328739135025553\n",
      "train loss:0.07195947014863534\n",
      "train loss:0.0389769427307803\n",
      "train loss:0.05995039933078975\n",
      "train loss:0.06661454929504301\n",
      "train loss:0.041730109948944855\n",
      "train loss:0.02211638167992336\n",
      "train loss:0.06193334369620595\n",
      "train loss:0.06329526916847755\n",
      "train loss:0.046278178893356905\n",
      "train loss:0.07623754787081177\n",
      "train loss:0.05269471399790758\n",
      "train loss:0.10202220117175488\n",
      "train loss:0.0830471592058753\n",
      "train loss:0.07394554905733768\n",
      "train loss:0.06835522218528062\n",
      "train loss:0.07081267209290573\n",
      "train loss:0.050611237293790544\n",
      "train loss:0.0434591528609889\n",
      "train loss:0.0473353071720883\n",
      "train loss:0.059861579136757534\n",
      "train loss:0.08721999468295635\n",
      "train loss:0.08080763052289378\n",
      "train loss:0.04317527750325082\n",
      "train loss:0.07232742690589006\n",
      "train loss:0.0373262747262654\n",
      "train loss:0.02173250794030921\n",
      "train loss:0.09436107869021829\n",
      "train loss:0.023215099972153257\n",
      "train loss:0.07478331997741397\n",
      "train loss:0.03384536365308322\n",
      "=== epoch:15, train acc:0.972, test acc:0.959 ===\n",
      "train loss:0.03567604608573191\n",
      "train loss:0.0611542237292215\n",
      "train loss:0.04667721417427018\n",
      "train loss:0.04476606193842405\n",
      "train loss:0.07476004809984158\n",
      "train loss:0.030606604852036902\n",
      "train loss:0.08342700828482835\n",
      "train loss:0.10189519379052664\n",
      "train loss:0.06975718865158874\n",
      "train loss:0.06197242438428581\n",
      "train loss:0.007700201956002945\n",
      "train loss:0.049435112261559944\n",
      "train loss:0.012543830555241156\n",
      "train loss:0.046353067585383975\n",
      "train loss:0.07820890344185095\n",
      "train loss:0.014275507048784246\n",
      "train loss:0.06291619342575719\n",
      "train loss:0.03410595886406872\n",
      "train loss:0.03127416824876116\n",
      "train loss:0.032590174559598936\n",
      "train loss:0.03395231058298273\n",
      "train loss:0.0320046487125663\n",
      "train loss:0.0413367411592055\n",
      "train loss:0.11902620853625315\n",
      "train loss:0.019660845410161113\n",
      "train loss:0.04865007892968287\n",
      "train loss:0.04346029861884343\n",
      "train loss:0.04953732288878666\n",
      "train loss:0.09719951620015602\n",
      "train loss:0.01555182079419865\n",
      "train loss:0.04210077143766362\n",
      "train loss:0.04511435688299516\n",
      "train loss:0.027964073460160766\n",
      "train loss:0.056869105728503115\n",
      "train loss:0.03996659139907757\n",
      "train loss:0.11010292104735879\n",
      "train loss:0.039119204501676\n",
      "train loss:0.11015452994488635\n",
      "train loss:0.06531327907722097\n",
      "train loss:0.02044358505721747\n",
      "train loss:0.04105003400908513\n",
      "train loss:0.08128030687414461\n",
      "train loss:0.07300140512812266\n",
      "train loss:0.023446989322896877\n",
      "train loss:0.07300326541883648\n",
      "train loss:0.05160216457321044\n",
      "train loss:0.03435190291827142\n",
      "train loss:0.025424631654431696\n",
      "train loss:0.04907219164894586\n",
      "train loss:0.04493840067297733\n",
      "=== epoch:16, train acc:0.983, test acc:0.956 ===\n",
      "train loss:0.05968372827795724\n",
      "train loss:0.07446611892385432\n",
      "train loss:0.05456350691384906\n",
      "train loss:0.05780709627449664\n",
      "train loss:0.05288947099628772\n",
      "train loss:0.022787365104405613\n",
      "train loss:0.05816873286991461\n",
      "train loss:0.04720730435109996\n",
      "train loss:0.05253615641905944\n",
      "train loss:0.03831751352411011\n",
      "train loss:0.06197555060894119\n",
      "train loss:0.03614330213761939\n",
      "train loss:0.0157635317088099\n",
      "train loss:0.041553180190729774\n",
      "train loss:0.026476728847102594\n",
      "train loss:0.0792819901723617\n",
      "train loss:0.031079540321502716\n",
      "train loss:0.02061207237363356\n",
      "train loss:0.018795230588823427\n",
      "train loss:0.02375840475170526\n",
      "train loss:0.05089831684994037\n",
      "train loss:0.014584831112061243\n",
      "train loss:0.05039792535508309\n",
      "train loss:0.038851139734337514\n",
      "train loss:0.04533332812935698\n",
      "train loss:0.019367115913371674\n",
      "train loss:0.05272715955373352\n",
      "train loss:0.038778809034017064\n",
      "train loss:0.036478505641060976\n",
      "train loss:0.04038593355762701\n",
      "train loss:0.06100479098483787\n",
      "train loss:0.0394370979611236\n",
      "train loss:0.008043606512853171\n",
      "train loss:0.024699462560829315\n",
      "train loss:0.036480490385759956\n",
      "train loss:0.04064078432639346\n",
      "train loss:0.07202476486353139\n",
      "train loss:0.012572908721951175\n",
      "train loss:0.04669723203341092\n",
      "train loss:0.015496173798940284\n",
      "train loss:0.022958281028384594\n",
      "train loss:0.0253228685187081\n",
      "train loss:0.05889249246801775\n",
      "train loss:0.027761799369589855\n",
      "train loss:0.058342359590981684\n",
      "train loss:0.07440195776776778\n",
      "train loss:0.08647887776450869\n",
      "train loss:0.0233027571508218\n",
      "train loss:0.06404196209362173\n",
      "train loss:0.017751447496791133\n",
      "=== epoch:17, train acc:0.987, test acc:0.959 ===\n",
      "train loss:0.03257794252622688\n",
      "train loss:0.028043856387549738\n",
      "train loss:0.04793721037228658\n",
      "train loss:0.014000310203791874\n",
      "train loss:0.019798697223080662\n",
      "train loss:0.026540387254384554\n",
      "train loss:0.019203011599735583\n",
      "train loss:0.037016501671519826\n",
      "train loss:0.024636158296412365\n",
      "train loss:0.01738557478769832\n",
      "train loss:0.033755419446152866\n",
      "train loss:0.08662029281435264\n",
      "train loss:0.026685241536620618\n",
      "train loss:0.012255316610174507\n",
      "train loss:0.0250860992293392\n",
      "train loss:0.02207858237327292\n",
      "train loss:0.032405801824234706\n",
      "train loss:0.033632564057714115\n",
      "train loss:0.017485004939730188\n",
      "train loss:0.08367796781399708\n",
      "train loss:0.08379907448295054\n",
      "train loss:0.009649262883250694\n",
      "train loss:0.014979640305234266\n",
      "train loss:0.01715954476903159\n",
      "train loss:0.034547831516320404\n",
      "train loss:0.0352855170564246\n",
      "train loss:0.02447768472078653\n",
      "train loss:0.023494062551551252\n",
      "train loss:0.036105715333455496\n",
      "train loss:0.019173360945900286\n",
      "train loss:0.015628732118277547\n",
      "train loss:0.06647111386478462\n",
      "train loss:0.024572459202204362\n",
      "train loss:0.017419054101740286\n",
      "train loss:0.02381135007650123\n",
      "train loss:0.052460156234877396\n",
      "train loss:0.04676299883822255\n",
      "train loss:0.09118854152247914\n",
      "train loss:0.02153474632800961\n",
      "train loss:0.012055510658215111\n",
      "train loss:0.06371621672563066\n",
      "train loss:0.018638931983152942\n",
      "train loss:0.01708647919249107\n",
      "train loss:0.05212496527989611\n",
      "train loss:0.014161345369268672\n",
      "train loss:0.07506832775273888\n",
      "train loss:0.028663244181600232\n",
      "train loss:0.023505991483892586\n",
      "train loss:0.030569343782866607\n",
      "train loss:0.021950325808625885\n",
      "=== epoch:18, train acc:0.991, test acc:0.962 ===\n",
      "train loss:0.02863078137107084\n",
      "train loss:0.045244631633239385\n",
      "train loss:0.026798249420056375\n",
      "train loss:0.014599292051772749\n",
      "train loss:0.014971362195743793\n",
      "train loss:0.03340156142310219\n",
      "train loss:0.018518753936081527\n",
      "train loss:0.042587685866846324\n",
      "train loss:0.023357171464538164\n",
      "train loss:0.017171535869405414\n",
      "train loss:0.011950994081075614\n",
      "train loss:0.06185926569605492\n",
      "train loss:0.02724035067703831\n",
      "train loss:0.03401309120308722\n",
      "train loss:0.046567532211506855\n",
      "train loss:0.03990991638194981\n",
      "train loss:0.04400995610836419\n",
      "train loss:0.02826948614006716\n",
      "train loss:0.023270913031534897\n",
      "train loss:0.030072995978355013\n",
      "train loss:0.0259422114487449\n",
      "train loss:0.061323693325722684\n",
      "train loss:0.007886268652433552\n",
      "train loss:0.01930901251083873\n",
      "train loss:0.020530432862136457\n",
      "train loss:0.02721739491563614\n",
      "train loss:0.032608495030828466\n",
      "train loss:0.027724956870742914\n",
      "train loss:0.0498567377800706\n",
      "train loss:0.004500607544159934\n",
      "train loss:0.02866981626545229\n",
      "train loss:0.011580865149108707\n",
      "train loss:0.10419380226027988\n",
      "train loss:0.0193695410012963\n",
      "train loss:0.03278214029282599\n",
      "train loss:0.07292209371959832\n",
      "train loss:0.01438907979398677\n",
      "train loss:0.017718694314719473\n",
      "train loss:0.036834177754153055\n",
      "train loss:0.04790076653908782\n",
      "train loss:0.0066200104395149695\n",
      "train loss:0.0056967160526729425\n",
      "train loss:0.10097870053804128\n",
      "train loss:0.029591005415352725\n",
      "train loss:0.026056363288802092\n",
      "train loss:0.02361055964522636\n",
      "train loss:0.01148892573105036\n",
      "train loss:0.024852111742472722\n",
      "train loss:0.03323169668400254\n",
      "train loss:0.019750682210537975\n",
      "=== epoch:19, train acc:0.992, test acc:0.956 ===\n",
      "train loss:0.0160940279694555\n",
      "train loss:0.03746388015966697\n",
      "train loss:0.03481037472107375\n",
      "train loss:0.048096504495083625\n",
      "train loss:0.03948298085894923\n",
      "train loss:0.06133654590852878\n",
      "train loss:0.032851546690434545\n",
      "train loss:0.026571945952455548\n",
      "train loss:0.03685063543661575\n",
      "train loss:0.02593171090946842\n",
      "train loss:0.023482699472095407\n",
      "train loss:0.02641582472542397\n",
      "train loss:0.027507360916864187\n",
      "train loss:0.014250027943199607\n",
      "train loss:0.013778264575501117\n",
      "train loss:0.014409314561875252\n",
      "train loss:0.019388516274765222\n",
      "train loss:0.015764459452169685\n",
      "train loss:0.02474892260104614\n",
      "train loss:0.026805000493819064\n",
      "train loss:0.028460993646865967\n",
      "train loss:0.02877819176994174\n",
      "train loss:0.020196367874344298\n",
      "train loss:0.04641342089031466\n",
      "train loss:0.02822561500307643\n",
      "train loss:0.028029866491767075\n",
      "train loss:0.017249016116272914\n",
      "train loss:0.02086647916380118\n",
      "train loss:0.01569851507759132\n",
      "train loss:0.025291293389502412\n",
      "train loss:0.02280243043637682\n",
      "train loss:0.014128619421582214\n",
      "train loss:0.022183978428232675\n",
      "train loss:0.015567997756564653\n",
      "train loss:0.017645303324502707\n",
      "train loss:0.05651036353163886\n",
      "train loss:0.013882838903107158\n",
      "train loss:0.012138974432718665\n",
      "train loss:0.006451085661542565\n",
      "train loss:0.007861790798778434\n",
      "train loss:0.018855271166159385\n",
      "train loss:0.009506352011041684\n",
      "train loss:0.010438088612354792\n",
      "train loss:0.019251009219072985\n",
      "train loss:0.025651563206020567\n",
      "train loss:0.028381614276740175\n",
      "train loss:0.018261727228098226\n",
      "train loss:0.022167976462874668\n",
      "train loss:0.015635380318092307\n",
      "train loss:0.013067879378441845\n",
      "=== epoch:20, train acc:0.99, test acc:0.961 ===\n",
      "train loss:0.022574618504621568\n",
      "train loss:0.032768984865720656\n",
      "train loss:0.0269506947529751\n",
      "train loss:0.011191577057767976\n",
      "train loss:0.014361154031228564\n",
      "train loss:0.033766291530150934\n",
      "train loss:0.02229033786050808\n",
      "train loss:0.03653076858367103\n",
      "train loss:0.02963336408574273\n",
      "train loss:0.012944520478706319\n",
      "train loss:0.03688716549101416\n",
      "train loss:0.02202107527576742\n",
      "train loss:0.03982279667878095\n",
      "train loss:0.04028889126901681\n",
      "train loss:0.01739867062418963\n",
      "train loss:0.03571897527933963\n",
      "train loss:0.01638970104792\n",
      "train loss:0.013864512871444771\n",
      "train loss:0.01793472322799503\n",
      "train loss:0.028207956370455776\n",
      "train loss:0.008790981450276326\n",
      "train loss:0.02676391952642673\n",
      "train loss:0.020393130819389996\n",
      "train loss:0.05774772016711781\n",
      "train loss:0.03100237819209293\n",
      "train loss:0.028964566320078343\n",
      "train loss:0.006667682532633747\n",
      "train loss:0.00870369865467554\n",
      "train loss:0.026834775862462457\n",
      "train loss:0.030736088169421208\n",
      "train loss:0.010375727297438707\n",
      "train loss:0.026937059333254267\n",
      "train loss:0.023544170964789356\n",
      "train loss:0.038829783998599765\n",
      "train loss:0.00861573643788809\n",
      "train loss:0.018070821852828725\n",
      "train loss:0.016453671224914236\n",
      "train loss:0.004950285814750202\n",
      "train loss:0.027408345124288705\n",
      "train loss:0.01305492702138518\n",
      "train loss:0.04392019009952338\n",
      "train loss:0.010727658456686755\n",
      "train loss:0.018216136620225455\n",
      "train loss:0.03541260717173693\n",
      "train loss:0.035721126372803724\n",
      "train loss:0.007881511079536756\n",
      "train loss:0.02860321665656473\n",
      "train loss:0.022724128479011542\n",
      "train loss:0.016035113637556272\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.962\n",
      "Saved Network Parameters!\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from dataset.mnist import load_mnist\n",
    "from ch07.simple_convnet import SimpleConvNet\n",
    "from common.trainer import Trainer\n",
    "\n",
    "# \n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\n",
    "\n",
    "#  \n",
    "x_train, t_train = x_train[:5000], t_train[:5000]\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# \n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# \n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
